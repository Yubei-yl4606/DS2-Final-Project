---
title: "Investigating indicators that can tell if the breast cancer is malignant project"
author: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
library(klaR)
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(pROC)
library(ranger)
library(vip)
library(corrplot)
library(cowplot)
library(gridGraphics)
library(lime)
```

```{r}
#read data and remove useless columns
bc = read.csv("data.csv") %>% 
     select(diagnosis, symmetry_mean, contains(c("texture")), smoothness_mean, smoothness_worst,
            symmetry_se, fractal_dimension_se, symmetry_worst, concavity_se, concave.points_se) %>%
        mutate(diagnosis = if_else(diagnosis == "B", "Benign", "Malignant")) %>% 
        mutate_at(vars(diagnosis), as.factor)
```
## Introduction:   
  Breast cancer is the cancer which develops from the breast tissue. It is the second leading cause of cancer death in women (the first leading cause of cancer death is the lung cancer). The death rate of the breast cancer is about 1 in 39 (American Cancer Society, 2021). The correct prediction of the cancer is benign or malignant could have significant impact on further decisions, such as further screening and preventative actions (Stark, Hart, Nartowt and Deng, 2019). In this report, we are trying to use about 11 different variables to predict the binary outcome (benign or malignant) of the diagnosis of breast cancer. We hope to build a model that performs well on predicting the nature of the breast cancer based on different predictors such as the mean radius, perimeter and the area of the tumor. 
  
### Data preparing: 
  The *Breast Cancer* dataset from Kaggle consists of 569 observations and 32 variables, including an ID variable, a diagnosis variable revealing the tumor status (benign or malignant), and other 30 different measurement variables. We have removed the ID column and the “X” column which both are meaningless in predicting. For these 30 predictors, they are generated by 10 types:
* radius  
* texture (standard deviation of gray-scale values)  
* perimeter  
* area  
* smoothness (local variation in radius lengths)  
* compactness (perimeterˆ2 / area - 1.0)  
* concavity (severity of concave portions of the contour)  
* concave points (number of concave portions of the contour)  
* symmetry  
* fractal dimension  
  The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. (Breast Cancer Wisconsin (Diagnostic) Data Set, 2021).       
  However, we only kept 11 of the 30 variables from the original dataset, because some of the variables are too informative. As the result, we kept the standard error of concave points, the standard error of fractal dimension, the standard error of symmetry, the standard error of concavity, the standard error of texture, the mean of smoothness, the mean of symmetry, the mean of texture, the worst of symmetry, the worst of texture, and the worst of smoothness as our predictors. Since these predictors are related to tumors, but they are not very informative and would not determine whether the tumor is malignant or not by themselves, we believe these 11 variables are good predictors for our prediction. In addition, we also want to investigate how accurate we can predict when we only have partial and less related information.  
  For data tidying, we simply converted variables diagnosis from class character into class factor for further analysis. Then, we have checked the number of missing values, and luckily, there are 0 missing value in our data. Furthermore, we have converted the outcome benign as 0 and malignant as 1 for convenience. We separated the data into the training(80%) and the testing(20%) sets. 

## Explotary Analysis/Visulization:
  We use exploratory analysis to further show the distribution, characteristics ,and interesting structure of the dataset. Several plots are used in this part.  

* Plot of distribution of the outcome variable  
* Plot of correlation  
* Plot of feature  

### Plot of distribution of the outcome variable  
```{r}
#Visualization
#Distribution of the outcome variable
palette_ro = c("#ee2f35", "#fa7211", "#fbd600", "#75c731", "#1fb86e", "#0488cf", "#7b44ab")
p1 <- ggplot(bc, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(stat = "Count", position = "stack", show.legend = FALSE) +
  scale_fill_manual(values = c(palette_ro[2], palette_ro[7])) +
  theme_minimal(base_size = 16) +
  geom_label(stat = "count", aes(label = ..count..), position = position_stack(vjust = 0.5),
             size = 5, show.legend = FALSE)
p1
```

From the plot above, it shows that among 569 observations, 357 observations are diagnosed as benign (about 62.7%) and 212 observations are diagnosed as malignant (about 37.3%).  

### Plot of correlation  
```{r}
#Correlation matrix
df_n <- bc %>%
  mutate_at(vars(diagnosis), as.double) %>%
  select(-diagnosis)
cor(df_n) %>%
  corrplot(method = "circle", type = "full", tl.col = "black")
```
From the correlation plot above, we can see that among 11 predictors, some of them are correlated with each other, suggesting that multi-collinearity is likely to occur. This may cause problems such as over-fitting and difficulties in interpretation. 

### Plot of feature  
```{r}
#Feature plot: normality assumption check
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = bc[, 2:7], 
            y = bc$diagnosis,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2),
            combine = FALSE)

##不知道怎么改字体大小 就分开写了两段code
#ggarrange(plots, nrow = 5)
#plots <- lapply(X = plots, FUN = function(x) x + theme(plot.title = element_text(size = 10)))
#cowplot::plot_grid(plotlist = plots, ncol=3)
```
```{r}
featurePlot(x = bc[, 8:12], 
            y = bc$diagnosis,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2),
            combine = FALSE)
```

The plots above show the distributions of all 11 predictors.  

## Models:
We have divided the dataset into the train set and the test set for further checking performance of the models.  
```{r}
set.seed(2021)
indexTrain <- createDataPartition(y = bc$diagnosis, p = 0.80, list = FALSE)
trainData <- bc[indexTrain, ]
testData <- bc[-indexTrain, ]
```

### Logistic regression & Penalized logistic regression
Since the outcome variable is binary and predictors are highly correlated with each other, we fit a logistic regression model and a penalized logistic regression model to the data.For logistic regression, the underlying assumption include: observations are independent of each other;little or no multicollinearity;linearity of independent variables and log odds. 

```{r}
# fit the glm model
glm.fit <- glm(diagnosis ~ .,
               data = trainData,
               family = binomial(link = "logit"))
```

```{r}
# check assumption of linear relationship between predictors and log odds/logit values（check发现violated了，最后可能就不放在report里了.....不然logistic的都做不了）

# Predict the probability of malignant tumor
probabilities <- predict(glm.fit, type = "response")

## Select only numeric predictors
mydata <- trainData %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata)
## Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
## Create the scatter plots:
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

```{r, warning=FALSE}
test.pred.prob1 <- predict(glm.fit, newdata = testData,
                          type = "response")

test.pred1 <- rep("Benign", length(test.pred.prob1))
test.pred1[test.pred.prob1>0.5] <- "Malignant"
confusionMatrix(data = as.factor(test.pred1),
                reference = testData$diagnosis,
                positive = "Malignant")
```

For testing data, confusion matrix gives us an accuracy of 87.61%. The p-value of accuracy greater than no information rate is almost zero(3.585e-09), thus the model is useful. Specificity(91.55%) is greater than sensitivity(80.95%). However, since data contains eleven predictors which is too many, we use regularization approach which help shrinkage unimportant coefficients by lambda, thus penalizes large number of predictors.

### Logistic regression with penalization (elastic net regularization)
```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-12, -2, length = 20)))
set.seed(1)
model.glmn <- train(x = trainData[2:12],
                    y = trainData$diagnosis,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)
plot(model.glmn, xTrans = function(x) log(x))
# test data AUC
glmn.pred = predict(model.glmn, newdata = testData, type = "prob")[,2]
roc.glmn <- roc(testData$diagnosis, glmn.pred)
roc.glmn$auc
```

```{r}
model.glmn$bestTune
```
For this penalized logistic regression model, the optimal tuning parameter being selected is 0.002.  I picked this value by setting a tuning grid from exp(-12) to exp(-2) and then uses cross validation to get the parameter which gives the optimal ROC curve.

For testing data, the area under ROC curve is 0.9202. The coefficients of predictors below provide us some information on how predictors affect the outcome variable: malignant or not.

```{r}
coef(model.glmn$finalModel, model.glmn$bestTune$lambda)
```

For one unit increase in standard error of the number of concave portions on cell contour, the odds ratio of having malignant tumor increases by 4.44e+166 (computed by e^383.72). For one unit decrease in standard error of fractal dimension, the odds ratio of having malignant tumor increases by 8.94e+139(computed by e^322.25). If we relate the coefficients to relative variable importance, we can conclude that variability of concave points are the most influential variable, whereas the mean of texture is the least influential variable since it's coefficient is the closest to zero.   

### Lasso regression:   
  Since many predictors are highly correlated to each other, we also fit a LASSO regression. The assumption of lasso model is the linear relationship between predictors and the odds.
```{r}
x = as.matrix(trainData[2:12])
y = trainData$diagnosis
set.seed(2021)
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(5, -5, length = 1000)),
                      family = "binomial")
cv.lasso$lambda.min
coef = predict(cv.lasso, s = "lambda.min", type = "coefficients")
lasso.pred = predict(cv.lasso, newx = as.matrix(testData[2:12]), s = "lambda.min", type = "response")
roc.lasso <- roc(testData$diagnosis, lasso.pred)
roc.lasso$auc
plot(roc.lasso, legacy.axes = TRUE, print.auc = TRUE)
legend("bottomright", legend = "LASSO") 
```
  For this LASSO regression model, the optimal tuning parameter being selected is 0.0067. We picked this value by setting a tuning grid from exp(5) to exp(-5). In this model, the parameters of the mean of symmetry and the worst of smoothness have been reduced to 0. Besides, the AUC is 0.9202.        
  For one unit increase in the mean of smoothness, there is an exp(58.797) increase in the odds ratio. For one unit increase in the standard error of symmetry, the is an exp(-96.616) decrease in the odds ratio. For one unit increase in the standard error of fractal dimension, the odds ratio decreases by exp(-287.63). For one unit increase in the standard error of concave points, the odds ratio increases by exp(319.33). Thus, we can conclude the standard error of concave points and the standard error of fractal dimension are the two most important variables.

### KNN 
```{r}
set.seed(2021)
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
model.knn <- train(x = bc[indexTrain,2:12],
                   y = bc$diagnosis[indexTrain],
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)
model.knn$bestTune
ggplot(model.knn, highlight = TRUE)
knn.pred <- predict(model.knn, newdata = bc[-indexTrain,], type = "prob")[,2]
roc.knn <- roc(bc$diagnosis[-indexTrain], knn.pred)
plot(roc.knn, legacy.axes = TRUE, print.auc = TRUE) #AUC is 0.923
plot(varImp(model.knn, scale = FALSE), top = 10, main = "knn")
```

### MARS
  Since the model could be nonlinear, we also fit the MARS model besides the random forest. Since MARS is a non-parametric method, there is no assumption made between the outcome and predictors.  
```{r}
mars_grid <- expand.grid(degree = 1:3, 
                         nprune = 2:20)
tuned_mars <- train(
  x = bc[indexTrain,2:12],
  y = bc$diagnosis[indexTrain],
  method = "earth",
  trControl = ctrl,
  tuneGrid = mars_grid)
ggplot(tuned_mars)
tuned_mars$bestTune
coef_mars = coef(tuned_mars$finalModel) 
plot(varImp(tuned_mars, scale = FALSE), top = 10, main = "mars")
predict_mars = predict(tuned_mars, newdata = testData, type = "prob")
roc_mars = roc(testData$diagnosis, predict_mars[,2])
roc_mars$auc
plot(roc_mars, legacy.axes = TRUE, print.auc = TRUE)
legend("bottomright", legend = "MARS")
```
We use the tune grid which degree from 1 to 3 and prune from 2 to 20 to choose the model with least cross validation, and the tune with 8 prunes and 1 degree has been chosen. The worst of texture and the standard error of concave points are the two most important variables. The AUC for this model is 0.925.

### Random Forest Model 
```{r}
ctrl_3 <- trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(2021)
rf.fit <- train(diagnosis ~ . , 
                bc, 
                subset = indexTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl_3)

ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = bc[-indexTrain,], type = "prob")[,1]
roc.rf <- roc(bc$diagnosis[-indexTrain], rf.pred)
plot(roc.rf, legacy.axes = TRUE, print.auc = TRUE) #AUC is 0.954



rf2.final.per <- ranger(diagnosis ~ . , 
                        bc[indexTrain,], 
                        mtry = rf.fit$bestTune[[1]], 
                        min.node.size = rf.fit$bestTune[[3]],
                        splitrule = "gini",
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))

explainer.rf = lime(bc[indexTrain, 2:12], rf.fit)
new_obs <- bc[-indexTrain,2:12][1:10,]
explanation.obs <- explain(new_obs,
                           explainer.rf, 
                           n_features = 11,
                           n_labels =2 )
plot_explanations(explanation.obs)
```

### SVM (radio kernal)
Support vector machines model is an efficient learning algorithms for non-linear functions. Because our final model could be non-linear, we train SVM model with radio kernel.We use radio kernel as our classifiers instead of using linear kernel because our data set is not fully separable.  

```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-8,0,len=10))) # tunes over both cost and sigma
set.seed(2021)
svmr.fit <- train(diagnosis ~ . ,
                  trainData,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
plot(svmr.fit, highlight = TRUE)
```
```{r}
svmr.fit$finalModel
```

```{r}
pred.svmr <- predict(svmr.fit, newdata = testData)
confusionMatrix(data = pred.svmr, 
                reference = testData$diagnosis)
```

For support vector machine with a radial kernel, training and testing errors are 0.096 and 0.106 respectively.

```{r}
#influential variable
explainer.svmr = lime(bc[indexTrain, 2:12], svmr.fit)
explanation.obs.svmr <- explain(new_obs,
                           explainer.svmr, 
                           n_features = 11,
                           n_labels =2 )
plot_explanations(explanation.obs.svmr)
```

For new observations, we can observe the importance of features by looking at their color for each case. As we can see, large value of 'standard error of concave points' is highly associated with malignant diagnosis, which corresponds to our conclusion from penalized logistic regression(elastic net). Moreover, worst symmetry is the second most influential variable associated with malignant diagnosis. 

### Discriminant Analysis: LDA  
LDA model assumes the predictors are drawn from the normal distribution, and the LDA further assumes equality of covariances among the predictor variables.
```{r}
# variance covariance matrix homogeneity check
ggplot(bc, aes(x = fractal_dimension_se, y = concave.points_se, col = diagnosis)) + 
    geom_point() + 
    stat_ellipse() + 
    scale_color_manual(values = c("blue", "red"))
```

Since 'fractal_dimension_se' and 'concave.points_se' are the two most important predictors, I select them to check the equal variance assumption. As we can see, the variance of diagnosis group Malignant is similar to group Benign because their patterns are equally dispersed. Moreover, LDA is pretty robust to normality assumption which is violated for many predictors, I decide to build linear discriminant analysis model using lda function. 

```{r}
set.seed(2021)
lda = lda(diagnosis ~., data = bc)
plot(lda)
predict_lda = predict(lda, newdata = testData, type = "prob")
roc_lda = roc(testData$diagnosis, predict_lda$posterior[,2])
roc_lda$auc
plot(roc_lda, legacy.axes = TRUE, print.auc = TRUE)
legend("bottomright", legend = "LDA")
```
The area under the ROC cureve is 0.936.    