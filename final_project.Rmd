---
title: "Investigating indicators that can tell if the breast cancer is malignant project"
author: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
library(klaR)
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(pROC)
library(corrplot)
library(cowplot)
library(gridGraphics)
```

```{r}
#read data and remove useless columns
bc = read.csv("data.csv") %>% 
     select(diagnosis, symmetry_mean, contains(c("texture")), smoothness_mean, smoothness_worst, symmetry_se, 
                  fractal_dimension_se, symmetry_worst, concavity_se, concave.points_se) %>% 
        mutate_at(vars(diagnosis), as.factor)%>% 
        mutate(diagnosis = if_else(diagnosis == "B", "Benign", "Malignant"),
               diagnosis = factor(diagnosis))
```
## Introduction:   
  Breast cancer is the cancer which develops from the breast tissue. It is the second leading cause of cancer death in women (the first leading cause of cancer death is the lung cancer). The death rate of the breast cancer is about 1 in 39 (American Cancer Society, 2021). The correct prediction of the cancer is benign or malignant could have significant impact on further decisions, such as further screening and preventative actions (Stark, Hart, Nartowt and Deng, 2019). In this report, we are trying to use about 11 different variables to predict the binary outcome (benign or malignant) of the diagnosis of breast cancer. We hope to build a model that performs well on predicting the nature of the breast cancer based on different predictors such as the mean radius, perimeter and the area of the tumor. 
  
### Data preparing: 
  The *Breast Cancer* dataset from Kaggle consists of 569 observations and 32 variables, including an ID variable, a diagnosis variable revealing the tumor status (benign or malignant), and other 30 different measurement variables. We have removed the ID column and the “X” column which both are meaningless in predicting. For these 30 predictors, they are generated by 10 types:
* radius  
* texture (standard deviation of gray-scale values)  
* perimeter  
* area  
* smoothness (local variation in radius lengths)  
* compactness (perimeterˆ2 / area - 1.0)  
* concavity (severity of concave portions of the contour)  
* concave points (number of concave portions of the contour)  
* symmetry  
* fractal dimension  
  The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. (Breast Cancer Wisconsin (Diagnostic) Data Set, 2021).       
  However, we only kept 11 of the 30 variables from the original dataset, because some of the variables are too informative. As the result, we kept the standard error of concave points, the standard error of fractal dimension, the standard error of symmetry, the standard error of concavity, the standard error of texture, the mean of smoothness, the mean of symmetry, the mean of texture, the worst of symmetry, the worst of texture, and the worst of smoothness as our predictors. Since these predictors are related to tumors, but they are not very informative and would not determine whether the tumor is malignant or not by themselves, we believe these 11 variables are good predictors for our prediction.
  For data tidying, we simply converted variables diagnosis from class character into class factor for further analysis. Then, we have checked the number of missing values, and luckily, there are 0 missing value in our data. Furthermore, we have converted the outcome benign as 0 and malignant as 1 for convenience. We separated the data into the training(80%) and the testing(20%) sets. 

## Explotary Analysis/Visulization:
  We use exploratory analysis to further show the distribution, characteristics ,and interesting structure of the dataset. Several plots are used in this part.  

* Plot of distribution of the outcome variable  
* Plot of correlation  
* Plot of feature  

### Plot of distribution of the outcome variable  
```{r}
#Visualization
#Distribution of the outcome variable
palette_ro = c("#ee2f35", "#fa7211", "#fbd600", "#75c731", "#1fb86e", "#0488cf", "#7b44ab")
p1 <- ggplot(bc, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(stat = "Count", position = "stack", show.legend = FALSE) +
  scale_fill_manual(values = c(palette_ro[2], palette_ro[7])) +
  theme_minimal(base_size = 16) +
  geom_label(stat = "count", aes(label = ..count..), position = position_stack(vjust = 0.5),
             size = 5, show.legend = FALSE)
p1
```

From the plot above, it shows that among 569 observations, 357 observations are diagnosed as benign (about 62.7%) and 212 observations are diagnosed as malignant (about 37.3%).  

### Plot of correlation  
```{r}
#Correlation matrix
df_n <- bc %>%
  mutate_at(vars(diagnosis), as.double) %>%
  select(-diagnosis)
cor(df_n) %>%
  corrplot(method = "circle", type = "full", tl.col = "black")
```
From the correlation plot above, we can see that among 11 predictors, some of them are correlated with each other, suggesting that multi-collinearity is likely to occur. This may cause problems such as over-fitting and difficulties in interpretation. 

### Plot of feature  
```{r}
#Feature plot: normality assumption check
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = bc[, 2:7], 
            y = bc$diagnosis,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2),
            combine = FALSE)

##不知道怎么改字体大小 就分开写了两段code
#ggarrange(plots, nrow = 5)
#plots <- lapply(X = plots, FUN = function(x) x + theme(plot.title = element_text(size = 10)))
#cowplot::plot_grid(plotlist = plots, ncol=3)
```
```{r}
featurePlot(x = bc[, 8:12], 
            y = bc$diagnosis,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2),
            combine = FALSE)
```

The plots above show the distributions of all 11 predictors.  

## Models:
We have divided the dataset into the train set and the test set for further checking performance of the models.  
```{r}
set.seed(2021)
indexTrain <- createDataPartition(y = bc$diagnosis, p = 0.80, list = FALSE)
trainData <- bc[indexTrain, ]
testData <- bc[-indexTrain, ]
```
### Lasso regression:   
```{r}
x = as.matrix(trainData[2:12])
y = trainData$diagnosis
set.seed(2021)
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(5, -5, length = 1000)),
                      family = "binomial")
cv.lasso$lambda.min
coef = predict(cv.lasso, s = "lambda.min", type = "coefficients")
lasso.pred = predict(cv.lasso, newx = as.matrix(testData[2:12]), s = "lambda.min", type = "response")
roc.lasso <- roc(testData$diagnosis, lasso.pred)
roc.lasso$auc
```

### Logistic regression with penalization: (待修改，就是glmnet的模型)
```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -2, length = 20)))
set.seed(1)
model.glmn <- train(x = trainData[2:11],
                    y = trainData$diagnosis,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)
plot(model.glmn, xTrans = function(x) log(x))
glmn.pred = predict(model.glmn, newdata = testData, type = "prob")[,2]
roc.glmn <- roc(testData$diagnosis, glmn.pred)
plot(varImp(model.glmn, scale = FALSE), top = 10, main = "glmnet")
roc.glmn$auc
```

### Discriminant Analysis: LDA (yubei)

```{r}
# variance covariance matrix homogeneity check
ggplot(bc, aes(x = fractal_dimension_se, y = concave.points_se, col = diagnosis)) + 
    geom_point() + 
    stat_ellipse() + 
    scale_color_manual(values = c("blue", "red"))
```

Since 'fractal_dimension_se' and 'concave.points_se' are the two most important predictors, I select them to check the equal variance assumption. As we can see, the variance of diagnosis group Malignant is similar to group Benign because their patterns are equally dispersed. Moreover, LDA is pretty robust to normality assumption which is violated for many predictors, I decide to build linear discriminant analysis model using caret package. 

```{r}
set.seed(2021)
model.lda <- train(x = trainData[2:12],
                   y = trainData$diagnosis,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
```